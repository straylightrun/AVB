{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adversarial Variational Bayes ##\n",
    "- https://arxiv.org/pdf/1701.04722.pdf\n",
    "- Use adversarial training along with a more flexible inference model \n",
    "- Reformulates VAE so that the noise is input into the model along with data\n",
    "- This allows model to learn arbitrary probability distributions in the latent space (unlike imposing a Gaussian for traditional VAEs)\n",
    "- They also introduce a discriminator which takes pairs of data and latent representation and tries to distinguish actual pairs from the data and generated pairs from the current model\n",
    "- The new optimization objective aims to maximize (over inference and generative parameters) the log probability of observing the data, $x$, given the learned latent representation, $z$, minus the discriminator, $T$'s, ability to tell that the pair $x, z$ were generated\n",
    "\n",
    "$$\\max_{\\theta,\\phi} \\mathbf{E}_{p_{D}(x)}\\mathbf{E}_{\\epsilon}\\left(-T^*(x, z_\\phi(x, \\epsilon)) + \\text{log} p_\\theta(x \\mid z_\\phi(x, \\epsilon) ) \\right) $$\n",
    "\n",
    "- $z_\\phi$ is the inference model\n",
    "- $\\epsilon$ is Gaussian noise\n",
    "- $p_\\theta$ is the generative model\n",
    "- $T^*$ is the optimal discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/stevenas/virtualenvs/py27gpu/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4765815740749994049, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15927646618\n",
       " locality {\n",
       "   bus_id: 2\n",
       " }\n",
       " incarnation: 15212392117987625565\n",
       " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import objectives\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.python.debug.lib import debug_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(18181)  # for reproducibility\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Simple NN implementation in TF\n",
    "\n",
    "def _build_layer(layer_input, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 name=None, \n",
    "                 activation=None):\n",
    "    \n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", \n",
    "                              [input_dim, output_dim], \n",
    "                              dtype=tf.float32,\n",
    "                              initializer=tf.random_normal_initializer(stddev=np.sqrt(2./input_dim), \n",
    "                                                                       seed=18181))\n",
    "    \n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", \n",
    "                             output_dim, \n",
    "                             dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    #print weights.name\n",
    "    x = tf.matmul(layer_input, weights) + biases\n",
    "    if activation is None:\n",
    "        return tf.nn.elu(x)\n",
    "    else:\n",
    "        return activation(x)\n",
    "    \n",
    "def build_deep_net(net_input, \n",
    "                   input_dim, \n",
    "                   hidden_dim, \n",
    "                   output_dim, \n",
    "                   n_layers, \n",
    "                   name,\n",
    "                   activation=None,\n",
    "                   parent_scope=False):\n",
    "    \n",
    "    assert(n_layers > 2)\n",
    "    \n",
    "    if parent_scope:\n",
    "        # Input layer\n",
    "        with tf.variable_scope(name + \"_input\", reuse=True):\n",
    "            current_input = _build_layer(net_input, input_dim, hidden_dim)\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(n_layers - 2):\n",
    "            with tf.variable_scope(name + \"_hidden\" + str(i), reuse=True):\n",
    "                current_input = _build_layer(current_input, hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        with tf.variable_scope(name + \"_output\", reuse=True):\n",
    "            # We only allow activation specification of the last layer\n",
    "            output = _build_layer(current_input, hidden_dim, output_dim, name=name, activation=activation)\n",
    "            \n",
    "    else:\n",
    "        # Create a new scope\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            # Input layer\n",
    "            with tf.variable_scope(name + \"_input\"):\n",
    "                current_input = _build_layer(net_input, input_dim, hidden_dim)\n",
    "\n",
    "            # Hidden layers\n",
    "            for i in range(n_layers - 2):\n",
    "                with tf.variable_scope(name + \"_hidden\" + str(i)):\n",
    "                    current_input = _build_layer(current_input, hidden_dim, hidden_dim)\n",
    "\n",
    "            # Output layer\n",
    "            with tf.variable_scope(name + \"_output\"):\n",
    "                output = _build_layer(current_input, hidden_dim, output_dim, name=name, activation=activation)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def build_layer(layer_input, input_dim, output_dim, name, activation=None):\n",
    "    # Single layer net\n",
    "    with tf.variable_scope(name):\n",
    "        return _build_layer(layer_input, input_dim, output_dim, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate test data \n",
    "n_pts = 15000\n",
    "\n",
    "def genSpiral(frac, noiseStd = 0.01, spins = 2):\n",
    "    x = frac*np.cos(2*np.pi*frac*spins) + np.random.normal(0, noiseStd)\n",
    "    y = frac*np.sin(2*np.pi*frac*spins) + np.random.normal(0, noiseStd)\n",
    "    return([x, y])\n",
    "\n",
    "# Generate three point clouds\n",
    "x_train = np.random.multivariate_normal((0,0), np.diag([1,1]), n_pts/3)\n",
    "x_train = np.concatenate([x_train, np.random.multivariate_normal((-10,-20), np.diag([1,1]), n_pts/3)])\n",
    "x_train = np.concatenate([x_train, np.random.multivariate_normal((30,20), np.diag([1,1]), n_pts/3)])\n",
    "np.random.shuffle(x_train)\n",
    "x_train = x_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Terenian/20.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data plot\n",
    "trace = go.Scatter(\n",
    "    x = x_train[:500,0],\n",
    "    y = x_train[:500,1],\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        size='4'#,\n",
    "        #color = x_train[indices, markers['CD3']], \n",
    "        #colorscale='Viridis',\n",
    "        #showscale=True\n",
    "    )\n",
    ")\n",
    "\n",
    "plt_data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "       range=[-40, 40]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[-40,40]\n",
    "   ),\n",
    "   height=800,\n",
    "   width=800,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=plt_data, layout=layout)\n",
    "py.iplot(fig, filename='avb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "original_dim = 2\n",
    "latent_dim = 1\n",
    "layers = 6 # >= 3\n",
    "batch_size = 200\n",
    "hidden_dim = 256\n",
    "epsilon_std = 1.\n",
    "clipNorm = 0.05\n",
    "M = 30 # number of noise vectors\n",
    "lr = 10**-6 # Learning Rate\n",
    "epochs = 5000\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, original_dim], name='x')\n",
    "z = tf.random_normal([batch_size, latent_dim], name=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Contrast ##\n",
    "- Using \"Adaptive Contrast\" from the paper\n",
    "    - Allows us to compare the current inference model to an adaptive distribution instead of the prior $p(z)$\n",
    "    - Estimate moments of inference model\n",
    "    - Generate $m$ noise vectors ($\\epsilon$)\n",
    "    - Each noise vector is passed through a small NN producing a vector with same dimensionality as latent space ... $v_{i,k}(\\epsilon)$\n",
    "    - Another set of NNs are used to estimate $m$ coefficients from the input $x$ .... $a_i(x)$\n",
    "    - The latent space representation is a linear combination of the learned noise vectors using these coefficients\n",
    "    \n",
    "    $$z_k = \\sum_{i=1}^m v_{i,k}(\\epsilon_i )a_{i,k}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adaptive distribution r_alpha(z | x)\n",
    "# Generate noise vector nets\n",
    "noise_basis = []\n",
    "\n",
    "for i in range(M):\n",
    "    noise_basis.append(build_deep_net(   tf.random_normal([batch_size, latent_dim], \n",
    "                                                          stddev=epsilon_std, \n",
    "                                                          dtype=tf.float32), \n",
    "                                         latent_dim, \n",
    "                                         128, \n",
    "                                         latent_dim, \n",
    "                                         16, \n",
    "                                         name=\"alpha_noise_basis_{}\".format(i)))\n",
    "    \n",
    "v_basis = tf.stack(noise_basis, axis=1, name=\"v_basis\")  \n",
    "a_phis = tf.expand_dims(build_deep_net( x, \n",
    "                                        original_dim, \n",
    "                                        hidden_dim, \n",
    "                                        M, \n",
    "                                        layers, \n",
    "                                        name=\"alpha_a\"), axis=1)\n",
    "\n",
    "# Moments estimated from minibatch - for each noise basis vector\n",
    "alpha_mini_mu = tf.reduce_mean(v_basis, axis=0, name=\"alpha_mini_mu\")\n",
    "alpha_mini_sigma = tf.reduce_mean( (v_basis - alpha_mini_mu)**2 , axis=0, name=\"alpha_mini_sigma\")\n",
    "\n",
    "# Estimate moments of current inference model for x\n",
    "alpha_mu_x = tf.reduce_sum(alpha_mini_mu * tf.transpose(a_phis, perm=[0,2,1]), axis=1, name=\"alpha_mu_x\")\n",
    "alpha_sigma_x = tf.reduce_sum(alpha_mini_sigma * tf.transpose(a_phis, perm=[0,2,1])**2, axis=1, name=\"alpha_sigma_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Encoder/Inference ##\n",
    "- Creates a latent representation of data\n",
    "\n",
    "$$q_\\phi\\left( z \\mid x\\right)$$\n",
    "\n",
    "- Using the reparameratization trick from Kingma and Welling...\n",
    "\n",
    "$$z_\\phi(x, \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inference model - q_phi(z | x)\n",
    "# Linear combination of the noise basis vectors from above\n",
    "z_phi = tf.squeeze(tf.matmul(a_phis, v_basis), axis=1, name=\"z_phi\") \n",
    "\n",
    "# Z-score transform it\n",
    "z_hat = tf.identity((z_phi - alpha_mu_x) / alpha_sigma_x, name=\"z_hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder/Generative ##\n",
    "- Reconstruct original from latent space representation\n",
    "$$p_\\theta(x \\mid z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decoder - P(x | z) - the output of this should be the original x\n",
    "p_theta = build_deep_net(z_hat, \n",
    "                         latent_dim, \n",
    "                         hidden_dim, \n",
    "                         original_dim, \n",
    "                         layers,\n",
    "                         activation=tf.identity,\n",
    "                         name=\"p_theta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator ##\n",
    "- Tries to distinguish generated from real pairs of $x$ and $z$\n",
    "- $\\text{z_hat}$ is the Z-score transformed version of $z$ generated by the inference model\n",
    "- That way, the discriminator just compares this to the multivariate Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discriminator - Same Network - Distinguish generated pairs (x,z) from real\n",
    "T_psi_generated = build_deep_net(tf.concat([x, z_hat], axis=1), \n",
    "                                           original_dim + latent_dim, \n",
    "                                           hidden_dim, \n",
    "                                           1, \n",
    "                                           layers, \n",
    "                                           name=\"T_psi\",\n",
    "                                           parent_scope=False)\n",
    "\n",
    "with tf.variable_scope(\"T_psi\"): # \"real\" input, but using same network as above - using adaptive contrast\n",
    "    T_psi_real = build_deep_net(tf.concat([x, z], axis=1), \n",
    "                                           original_dim + latent_dim, \n",
    "                                           hidden_dim, \n",
    "                                           1, \n",
    "                                           layers, \n",
    "                                           name=\"T_psi\",\n",
    "                                           parent_scope=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Optimizer ##\n",
    "- These are negated from the paper so we can apply gradient descent\n",
    "- We use MSE of receonstruction for $p(x\\mid z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_theta = objectives.mean_squared_error(x, p_theta) # P(x|z) should be high (so minimize recon error)\n",
    "loss_phi = T_psi_generated + loss_theta # Phi is the inference network. We want to minimize the \n",
    "                                        # reconstruction error and the discriminator's ability to distinguish\n",
    "                                        # real from generated samples\n",
    "\n",
    "# This is original since it's subtracted in paper\n",
    "# Added the squared value loss to regularize and to prevent saturation of either\n",
    "loss_psi =  tf.log(tf.sigmoid(T_psi_generated) + 10**-5) + \\\n",
    "            tf.log(1 - tf.sigmoid(T_psi_real) + 10**-5) \n",
    "            #0.1*tf.square(T_psi_generated) + \\\n",
    "            #0.1*tf.square(T_psi_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_apply_grad(loss, keyword=None, learn_rate=0.001, clip_norm=0.01):\n",
    "    \"\"\" Computes the gradients of variables matching keyword. Returns an optimizer\"\"\"\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "    if keyword is None:\n",
    "        cur_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]\n",
    "    else:\n",
    "        cur_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if keyword in v.name]\n",
    "    g = opt.compute_gradients(loss, cur_vars)\n",
    "    g = [(tf.clip_by_norm(gr, clip_norm), v) for gr, v in g]\n",
    "    optimize = opt.apply_gradients(g)\n",
    "    return optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_all = compute_apply_grad(loss_theta + loss_phi + loss_psi, learn_rate=lr, clip_norm=clipNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_theta = tf.reduce_mean(loss_theta) # Over batch\n",
    "avg_phi = tf.reduce_mean(loss_phi) \n",
    "avg_psi = tf.reduce_mean(loss_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 theta(generative)=201.10, phi(inference)=200.10, psi(discrimination)=-1.63\n",
      "Epoch: 0021 theta(generative)=171.37, phi(inference)=170.40, psi(discrimination)=-2.08\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "for e in range(epochs):\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        ex = x_train[i:i+batch_size,:]\n",
    "        \n",
    "        _, at, ap, aps = sess.run([optimizer_all,\n",
    "                                   avg_theta,\n",
    "                                   avg_phi,\n",
    "                                   avg_psi],\n",
    "                                  feed_dict={x: ex})            \n",
    "        ctr += 1\n",
    "    if e % 20 == 0:    \n",
    "        statusStr = \"theta(generative)={:.2f}, phi(inference)={:.2f}, psi(discrimination)={:.2f}\"\n",
    "        print \"Epoch:\", '%04d' % (e+1), statusStr.format(at, ap, aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Terenian/20.embed\" height=\"800px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on some training data\n",
    "x_sample = x_train[np.random.choice(x_train.shape[0], batch_size, replace=False), :]\n",
    "pred_z, pred_x = sess.run([z_phi, p_theta], feed_dict={x: x_sample})\n",
    "\n",
    "# Create a trace\n",
    "pred = go.Scatter(\n",
    "    x = pred_x[:,0],\n",
    "    y = pred_x[:,1],\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        size='4'\n",
    "    )\n",
    ")\n",
    "\n",
    "orig = go.Scatter(\n",
    "    x = x_sample[:,0],\n",
    "    y = x_sample[:,1],\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        size='4'\n",
    "    )\n",
    ")\n",
    "\n",
    "z_plt = go.Scatter(\n",
    "    x = pred_z,\n",
    "    y = np.repeat(0, pred_z.shape[0]),\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        size='4'\n",
    "    )\n",
    ")\n",
    "\n",
    "plt_data = [pred, orig, z_plt]\n",
    "\n",
    "layout = go.Layout(\n",
    "   height=800,\n",
    "   width=800,\n",
    "   xaxis=dict(\n",
    "       range=[-40, 40]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[-40,40]\n",
    "   )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=plt_data, layout=layout)\n",
    "py.iplot(fig, filename='avb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
